
@article{kingma_adam:_2014,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2018-12-28},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:C\:\\Users\\jaspe\\Zotero\\storage\\HTWA6MVY\\Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\THIP9W2E\\1412.html:text/html}
}

@article{gal_theoretically_2015,
	title = {A {Theoretically} {Grounded} {Application} of {Dropout} in {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1512.05287},
	abstract = {Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.},
	urldate = {2018-12-28},
	journal = {arXiv:1512.05287 [stat]},
	author = {Gal, Yarin and Ghahramani, Zoubin},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.05287},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1512.05287 PDF:C\:\\Users\\jaspe\\Zotero\\storage\\KJB6M3CW\\Gal and Ghahramani - 2015 - A Theoretically Grounded Application of Dropout in.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\QX6ZAA28\\1512.html:text/html}
}

@article{bird_phonology_2005,
	title = {Phonology},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-1},
	doi = {10.1093/oxfordhb/9780199276349.013.0001},
	abstract = {This article presents the fundamentals of descriptive phonology and gives an overview of computational phonology. Phonology is the systematic study of sounds used in language, and their composition into syllables, words, and phrases. It introduces some of the key concepts of phonology by simple examples involving real data and gives a brief discussion of early generative phonology. It analyses the autosegmental phonology using some data from African tone language. This article considers in detail one level of phonological hierarchy, namely, the syllable. It reveals many interesting issues that are confronted by phonological analysis. Some of these theoretical frameworks include: lexical phonology, underspecification phonology, government phonology, declarative phonology, and optimality theory. The article provides a means for phonological generalizations such as rules and constraints to give a finite-state interpretation.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Bird, Steven},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\YU8TBC5V\\oxfordhb-9780199276349-e-1.html:text/html}
}

@article{trost_morphology_2005,
	title = {Morphology},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-2},
	doi = {10.1093/oxfordhb/9780199276349.013.0002},
	abstract = {This article discusses in detail computational morphology with examples from various languages. It deals with the processing of words in both their graphemic, i.e. written, and their phonemic, i.e. spoken form. It has a wide range of practical applications such as spelling correction or automated hyphenation. It further seeks the fact that these tasks may seem simple to a human but they pose hard problems to a computer program. This article provides insights into why this is so and what techniques are available to tackle these tasks. It discusses the sort of information that is expressed by morphology and differs widely between languages and looks at the constraints involved in morphotactics. It is responsible for governing the rules for the combination of morphs into larger entities. It concludes with an outline of finite-state morphology and alternative formalisms.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Trost, Harald},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\EJ2CIAQ4\\oxfordhb-9780199276349-e-2.html:text/html}
}

@article{samuelsson_statistical_2005,
	title = {Statistical {Methods}},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-19},
	doi = {10.1093/oxfordhb/9780199276349.013.0019},
	abstract = {Statistical methods now belong to mainstream natural language processing. They have been successfully applied to virtually all tasks within language processing and neighbouring fields, including part-of-speech tagging, syntactic parsing, semantic interpretation, lexical acquisition, machine translation, information retrieval, and information extraction and language learning. This article reviews mathematical statistics and applies it to language modelling problems, leading up to the hidden Markov model and maximum entropy model. The real strength of maximum-entropy modelling lies in combining evidence from several rules, each one of which alone might not be conclusive, but which taken together dramatically affect the probability. Maximum-entropy modelling allows combining heterogeneous information sources to produce a uniform probabilistic model where each piece of information is formulated as a feature. The key ideas of mathematical statistics are simple and intuitive, but tend to be buried in a sea of mathematical technicalities. Finally, the article provides mathematical detail related to the topic of discussion.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Samuelsson, Christer},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\7VMB9GR7\\oxfordhb-9780199276349-e-19.html:text/html}
}

@article{mooney_machine_2005,
	title = {Machine {Learning}},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-20},
	doi = {10.1093/oxfordhb/9780199276349.013.0020},
	abstract = {This article introduces the type of symbolic machine learning in which decision trees, rules, or case-based classifiers are induced from supervised training examples. It describes the representation of knowledge assumed by each of these approaches and reviews basic algorithms for inducing such representations from annotated training examples and using the acquired knowledge to classify future instances. Machine learning is the study of computational systems that improve performance on some task with experience. Most machine learning methods concern the task of categorizing examples described by a set of features. These techniques can be applied to learn knowledge required for a variety of problems in computational linguistics ranging from part-of-speech tagging and syntactic parsing to word-sense disambiguation and anaphora resolution. Finally, this article reviews the applications to a variety of these problems, such as morphology, part-of-speech tagging, word-sense disambiguation, syntactic parsing, semantic parsing, information extraction, and anaphora resolution.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Mooney, Raymond J.},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\XVEP9JJQ\\oxfordhb-9780199276349-e-20.html:text/html}
}

@article{hirschman_evaluation_2005,
	title = {Evaluation},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-22},
	doi = {10.1093/oxfordhb/9780199276349.013.0022},
	abstract = {The commercial success of natural language (NL) technology has raised the technical criticality of evaluation. Choices of evaluation methods depend on software life cycles, typically charting four stages — research, advance prototype, operational prototype, and product. At the prototype stage, embedded evaluation can prove helpful. Analysis components can be loose grouped viz., segmentation, tagging, extracting information, and document threading. Output technologies such as text summarization can be evaluated in terms of intrinsic and extrinsic measures, the former checking for quality and informativeness and the latter, for efficiency and acceptability, in some tasks. ‘Post edit measures’ commonly used in machine translation, determine the amount of correction required to obtain a desirable output. Evaluation of interactive systems typically evaluates the system and the user as one team and deploys subject variability, which runs enough subjects to obtain statistical validity hence, incurring substantial costs. Evaluation being a social activity, creates a community for internal technical comparison, via shared evaluation criteria.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Hirschman, Lynette and Mani, Inderjeet},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\Y5XCQ4EI\\oxfordhb-9780199276349-e-22.html:text/html}
}

@article{hutchins_machine_2005,
	title = {Machine {Translation}: {General} {Overview}},
	shorttitle = {Machine {Translation}},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-27},
	doi = {10.1093/oxfordhb/9780199276349.013.0027},
	abstract = {This article generally appraises the process of computerized translation/machine translation (MT), with/without human support. Hence, the distinction between human-aided machine translation (HAMT) and machine-aided human translation (MAHT). The latter comprises software translation tools, providing access to online dictionaries, remote terminology databanks etc., which consolidate as “translation workstations”. All of these elements consolidate under the umbrella of computer-aided translation (CAT). MTs can be qualitatively enhanced by incorporation of pre-edited input texts to indicateprefixe, suffixes, clause boundaries etc. MT systems can be created specifically for two languages or more than one pair of languages. Bilingual systems can either operate in one direction — from one source language (SL) to one target language (TL), or in a to and fro manner. Further enhancement can be achieved by addition of knowledge-based systems, to resolve non-linguistic problems of ascertaining.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Hutchins, John},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\2B2M4WDB\\oxfordhb-9780199276349-e-27.html:text/html}
}

@article{somers_machine_2005,
	title = {Machine {Translation}: {Latest} {Developments}},
	shorttitle = {Machine {Translation}},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-28},
	doi = {10.1093/oxfordhb/9780199276349.013.0028},
	abstract = {This article attempts to locate MT (machine translation) in the contemporary context, while identifying its recent trends and themes. The 1990s were marked by the growing clout of empirical approaches, using increasingly available amounts of raw data in the form of parallel corpora. An appropriate instance is statistical MT, depending on bilingual corpus, but wherein, the translation depends on statistical modeling of the word order of target language and of source-target word equivalences. Example-based MT involves matching inputs against actual databases and identifying close matches. Successful MT, especially when attempting translation into languages marking gender of pronouns, those that have zero-anaphora contents, interpretation of anaphora assumes paramountcy. Regarding spoken-language MTs, coupling speech-to-text front-end and text-to-speech, the other way, is inadequate other than rigidly formal languages. More versatile operations such as dealing with dialogues, would involve greater complexities.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Somers, Harold},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\T47L54WD\\oxfordhb-9780199276349-e-28.html:text/html;Submitted Version:C\:\\Users\\jaspe\\Zotero\\storage\\2U4Z4S79\\Somers - 2005 - Machine Translation Latest Developments.pdf:application/pdf}
}

@article{tzoukermann_information_2005,
	title = {Information {Retrieval}},
	url = {http://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199276349.001.0001/oxfordhb-9780199276349-e-29},
	doi = {10.1093/oxfordhb/9780199276349.013.0029},
	abstract = {Information retrieval (IR) involves retrieving information from stored data, through user queries or pre-formulated user profiles. The information can be in any format. IR typically advances over four broad stages viz., identification of text types, document preprocessing, document indexing, and query processing and matching the same to documents. Although NLP has a role to play in IR, the procedural complexities of the latter impede determination of the stage of incorporation of the former into the latter. Earliest attempts at connecting NLP with IR, were extremely ambitious, proposing concepts instead of terms, as complex structures, to be compared using sophisticated algorithms. In its current state, IR still comes in handy, to retrieve information from various thesauri and ontologies, both in general-purpose lexical databases, as well as those categorizing knowledge in particular scientific and trade domains. However, NLP has yet to prove a better compatibility with IR, in enhancing the latter.},
	language = {en},
	urldate = {2018-12-28},
	journal = {The Oxford Handbook of Computational Linguistics},
	author = {Tzoukermann, Evelyne and Klavans, Judith L. and Strzalkowski, Tomek},
	month = jan,
	year = {2005},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\USCA59GE\\oxfordhb-9780199276349-e-29.html:text/html}
}

@article{hochreiter_long_1997,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
	number = {8},
	urldate = {2018-12-28},
	journal = {Neural Comput.},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780}
}

@article{schuster_bidirectional_1997,
	title = {Bidirectional recurrent neural networks},
	volume = {45},
	issn = {1053-587X},
	doi = {10.1109/78.650093},
	abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.},
	number = {11},
	journal = {IEEE Transactions on Signal Processing},
	author = {Schuster, M. and Paliwal, K. K.},
	month = nov,
	year = {1997},
	keywords = {artificial data, Artificial neural networks, bidirectional recurrent neural networks, classification experiments, complete symbol sequences, conditional posterior probability, Control systems, Databases, learning by example, learning from examples, negative time direction, Parameter estimation, pattern classification, phonemes, positive time direction, Probability, real data, recurrent neural nets, Recurrent neural networks, regression experiments, regular recurrent neural network, Shape, speech processing, speech recognition, Speech recognition, statistical analysis, Telecommunication control, TIMIT database, training, Training data},
	pages = {2673--2681},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\jaspe\\Zotero\\storage\\T5ACNZYU\\650093.html:text/html;Submitted Version:C\:\\Users\\jaspe\\Zotero\\storage\\LC8KXGE4\\Schuster and Paliwal - 1997 - Bidirectional recurrent neural networks.pdf:application/pdf}
}

@misc{noauthor_arpabet_2018,
	title = {{ARPABET}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=ARPABET&oldid=868829232},
	abstract = {ARPABET (also spelled ARPAbet) is a set of phonetic transcription codes developed by Advanced Research Projects Agency (ARPA) as a part of their Speech Understanding Research project in the 1970s. It represents phonemes and allophones of General American English with distinct sequences of ASCII characters. Two systems, one representing each segment with one character (alternating upper- and lower-case letters) and the other with two or more (case-insensitive), were devised, the latter being far more widely adopted.ARPABET has been used in several speech synthesizers, including Computalker for the S-100 system, SAM for the Commodore 64, SAY for the Amiga and TextAssist for the PC and Speakeasy from Intelligent Artefacts which used the Votrax SC01 speech synthesiser IC. It is also used in the CMU Pronouncing Dictionary. A revised version of ARPABET is used in the TIMIT corpus.},
	language = {en},
	urldate = {2018-12-28},
	journal = {Wikipedia},
	month = nov,
	year = {2018},
	note = {Page Version ID: 868829232},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\N986JBDP\\index.html:text/html}
}

@misc{carnegie_mellon_university_cmu_nodate,
	title = {The {CMU} {Pronouncing} {Dictionary}},
	url = {http://www.speech.cs.cmu.edu/cgi-bin/cmudict?stress=-s&in=CITATION},
	urldate = {2018-12-28},
	author = {{Carnegie Mellon University}},
	file = {The CMU Pronouncing Dictionary:C\:\\Users\\jaspe\\Zotero\\storage\\QH856D59\\cmudict.html:text/html}
}

@misc{noauthor_cmu_2018,
	title = {{CMU} {Pronouncing} {Dictionary}},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=CMU_Pronouncing_Dictionary&oldid=871723674},
	abstract = {The CMU Pronouncing Dictionary (also known as CMUdict) is an open-source pronouncing dictionary originally created by the Speech Group at Carnegie Mellon University (CMU) for use in speech recognition research.
CMUdict provides a mapping orthopraphic/phonetic for English words in their North American pronunciations. It is commonly used to generate representations for speech recognition (ASR), e.g. the CMU Sphinx system, and speech synthesis (TTS), e.g. the Festival system. CMUdict can be used as a training corpus for building statistical grapheme-to-phoneme (g2p) models that will generate pronunciations for words not yet included in the dictionary.
The most recent release is 0.7b; it contains over 134,000 entries. An interactive lookup version is available.},
	language = {en},
	urldate = {2018-12-28},
	journal = {Wikipedia},
	month = dec,
	year = {2018},
	note = {Page Version ID: 871723674},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\XL9MPKL7\\index.html:text/html}
}

@misc{noauthor_framewise_nodate,
	title = {Framewise phoneme classification with bidirectional {LSTM} and other neural network architectures - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608005001206},
	urldate = {2018-12-28},
	file = {Framewise phoneme classification with bidirectional LSTM and other neural network architectures - ScienceDirect:C\:\\Users\\jaspe\\Zotero\\storage\\M3BANU93\\S0893608005001206.html:text/html}
}

@misc{wikipedia_wikipedia:lists_2018,
	title = {Wikipedia:{Lists} of common misspellings/{For} machines},
	copyright = {Creative Commons Attribution-ShareAlike License},
	shorttitle = {Wikipedia},
	url = {https://en.wikipedia.org/w/index.php?title=Wikipedia:Lists_of_common_misspellings/For_machines&oldid=838176804},
	language = {en},
	urldate = {2018-12-28},
	journal = {Wikipedia},
	author = {{wikipedia}},
	month = apr,
	year = {2018},
	note = {Page Version ID: 838176804},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\7EP4BJFI\\index.html:text/html}
}

@misc{xiong_toolkit_2018,
	title = {Toolkit converting pronunciation in enwiktionary xml dump to cmudict format: abuccts/wikt2pron},
	copyright = {BSD-2-Clause},
	shorttitle = {Toolkit converting pronunciation in enwiktionary xml dump to cmudict format},
	url = {https://github.com/abuccts/wikt2pron},
	urldate = {2018-12-28},
	author = {Xiong, Yifan},
	month = oct,
	year = {2018},
	note = {original-date: 2017-06-06T05:47:28Z}
}

@misc{wikipedia_wikimedia_nodate,
	type = {Dataset},
	title = {Wikimedia downloads},
	url = {https://dumps.wikimedia.org/enwiktionary/20181101/enwiktionary-20181101-pages-meta-current.xml.bz2},
	urldate = {2018-12-28},
	journal = {Index of /enwiktionary/},
	author = {{Wikipedia}},
	file = {Index of /enwiktionary/:C\:\\Users\\jaspe\\Zotero\\storage\\4Q5T6Z6T\\enwiktionary.html:text/html}
}

@inproceedings{khoury_phonetic_2015,
	title = {Phonetic normalization of microtext},
	doi = {10.1145/2808797.2809352},
	abstract = {Microtext normalization is the challenge of discovering the English words corresponding to the unusually-spelled words used in social-media messages and posts. In this paper, we propose a novel method for doing this by rendering both English and microtext words phonetically based on their spelling, and matching similar ones together. We present our algorithm to learn spelling-to-phonetic probabilities and to efficiently search the English language and match words together. Our results demonstrate that our system correctly handles many types of normalization problems.},
	booktitle = {2015 {IEEE}/{ACM} {International} {Conference} on {Advances} in {Social} {Networks} {Analysis} and {Mining} ({ASONAM})},
	author = {Khoury, R.},
	month = aug,
	year = {2015},
	keywords = {Training data, Data mining, English language, English words, learning (artificial intelligence), Media, microtext, microtext phonetic normalization, natural language processing, normalization, phonetic, probability, Rendering (computer graphics), social media, social networking (online), social-media messages, social-media post, spelling-to-phonetic probability learning, text analysis, Training, Twitter, unusually-spelled words, wiktionary, word matching},
	pages = {1600--1601},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\jaspe\\Zotero\\storage\\2QAMWC37\\7403761.html:text/html}
}

@inproceedings{jahjah_word_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Word {Normalization} {Using} {Phonetic} {Signatures}},
	isbn = {978-3-319-34111-8},
	abstract = {Text normalization is the challenge of discovering the English words corresponding to the unusually-spelled words used in social-media messages and posts. In this paper, we detail a new word-searching strategy based on the idea of sounding out the consonants of the word. We describe our algorithm to extract the base consonant information from both miswritten and real words using a spelling and a phonetic approach. We then explain how this information is used to match similar words together. This strategy is shown to be time efficient as well as capable of correctly handling many types of normalization problems.},
	language = {en},
	booktitle = {Advances in {Artificial} {Intelligence}},
	publisher = {Springer International Publishing},
	author = {Jahjah, Vincent and Khoury, Richard and Lamontagne, Luc},
	editor = {Khoury, Richard and Drummond, Christopher},
	year = {2016},
	keywords = {Normalization, Social media, TheFreeDictionary, Wiktionary},
	pages = {180--185}
}

@inproceedings{wang_beam-search_2013,
	address = {Atlanta, Georgia},
	title = {A {Beam}-{Search} {Decoder} for {Normalization} of {Social} {Media} {Text} with {Application} to {Machine} {Translation}},
	url = {http://aclweb.org/anthology/N13-1050},
	urldate = {2018-12-28},
	booktitle = {Proceedings of the 2013 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Pidong and Ng, Hwee Tou},
	year = {2013},
	pages = {471--481},
	file = {Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\UVZLMRAY\\Wang and Ng - 2013 - A Beam-Search Decoder for Normalization of Social .pdf:application/pdf}
}

@inproceedings{aw_phrase-based_2006,
	address = {Stroudsburg, PA, USA},
	series = {{COLING}-{ACL} '06},
	title = {A {Phrase}-based {Statistical} {Model} for {SMS} {Text} {Normalization}},
	url = {http://dl.acm.org/citation.cfm?id=1273073.1273078},
	abstract = {Short Messaging Service (SMS) texts behave quite differently from normal written texts and have some very special phenomena. To translate SMS texts, traditional approaches model such irregularities directly in Machine Translation (MT). However, such approaches suffer from customization problem as tremendous effort is required to adapt the language model of the existing translation system to handle SMS text style. We offer an alternative approach to resolve such irregularities by normalizing SMS texts before MT. In this paper, we view the task of SMS normalization as a translation problem from the SMS language to the English language and we propose to adapt a phrase-based statistical MT model for the task. Evaluation by 5-fold cross validation on a parallel SMS normalized corpus of 5000 sentences shows that our method can achieve 0.80702 in BLEU score against the baseline BLEU score 0.6958. Another experiment of translating SMS texts from English to Chinese on a separate SMS text corpus shows that, using SMS normalization as MT preprocessing can largely boost SMS translation performance from 0.1926 to 0.3770 in BLEU score.},
	urldate = {2018-12-28},
	booktitle = {Proceedings of the {COLING}/{ACL} on {Main} {Conference} {Poster} {Sessions}},
	publisher = {Association for Computational Linguistics},
	author = {Aw, AiTi and Zhang, Min and Xiao, Juan and Su, Jian},
	year = {2006},
	pages = {33--40},
	file = {ACM Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\DDQGKY2D\\Aw et al. - 2006 - A Phrase-based Statistical Model for SMS Text Norm.pdf:application/pdf}
}

@misc{chollet_keras_2015,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, François},
	year = {2015}
}

@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2018-12-28},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1406.1078 PDF:C\:\\Users\\jaspe\\Zotero\\storage\\V5XZSC6Z\\Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\S3ZQU8JV\\1406.html:text/html}
}

@article{doval_performance_2018,
	title = {On the performance of phonetic algorithms in microtext normalization},
	volume = {113},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417418304305},
	doi = {10.1016/j.eswa.2018.07.016},
	abstract = {User–generated content published on microblogging social networks constitutes a priceless source of information. However, microtexts usually deviate from the standard lexical and grammatical rules of the language, thus making its processing by traditional intelligent systems very difficult. As an answer, microtext normalization consists in transforming those non–standard microtexts into standard well–written texts as a preprocessing step, allowing traditional approaches to continue with their usual processing. Given the importance of phonetic phenomena in non–standard text formation, an essential element of the knowledge base of a normalizer would be the phonetic rules that encode these phenomena, which can be found in the so–called phonetic algorithms. In this work we experiment with a wide range of phonetic algorithms for the English language. The aim of this study is to determine the best phonetic algorithms within the context of candidate generation for microtext normalization. In other words, we intend to find those algorithms that taking as input non–standard terms to be normalized allow us to obtain as output the smallest possible sets of normalization candidates which still contain the corresponding target standard words. As it will be stated, the choice of the phonetic algorithm will depend heavily on the capabilities of the candidate selection mechanism which we usually find at the end of a microtext normalization pipeline. The faster it can make the right choices among big enough sets of candidates, the more we can sacrifice on the precision of the phonetic algorithms in favour of coverage in order to increase the overall performance of the normalization system.},
	urldate = {2018-12-28},
	journal = {Expert Systems with Applications},
	author = {Doval, Yerai and Vilares, Manuel and Vilares, Jesús},
	month = dec,
	year = {2018},
	keywords = {Twitter, Fuzzy matching, Microtext normalization, Phonetic algorithm, Texting},
	pages = {213--222},
	file = {ScienceDirect Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\F9WQUNQW\\S0957417418304305.html:text/html}
}

@article{zhang_segmenting_2017,
	title = {Segmenting {Chinese} {Microtext}: {Joint} {Informal}-{Word} {Detection} and {Segmentation} with {Neural} {Networks}},
	shorttitle = {Segmenting {Chinese} {Microtext}},
	url = {https://www.ijcai.org/proceedings/2017/591},
	abstract = {Electronic proceedings of IJCAI 2017},
	urldate = {2018-12-28},
	author = {Zhang, Meishan and Fu, Guohong and Yu, Nan},
	year = {2017},
	pages = {4228--4234},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\AFVKDELJ\\591.html:text/html}
}

@inproceedings{xue_normalizing_2011,
	title = {Normalizing {Microtext}.},
	abstract = {The use of computer mediated communication has resulted in a new form of written text - Microtext - which is very different from well-written text. Tweets and SMS messages, which have limited length and may contain misspellings, slang, or abbreviations, are two typical examples of microtext. Micro-text poses new challenges to standard natural language processing tools which are usually designed for well-written text. The objective of this work is to normalize microtext, in order to produce text that could be suitable for further treatment. We propose a normalization approach based on the source channel model, which incorporates four factors, namely an orthographic factor, a phonetic factor, a contextual factor and acronym expansion. Experiments show that our approach can normalize Twitter messages reasonably well, and it outperforms existing algorithms on a public SMS data set. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.},
	author = {Xue, Zhenzhen and Yin, Dawei and Davison, Brian},
	month = jan,
	year = {2011},
	file = {Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\NTZ2HNVX\\Xue et al. - 2011 - Normalizing Microtext..pdf:application/pdf}
}

@inproceedings{li_normalization_2012,
	title = {Normalization of {Text} {Messages} {Using} {Character}- and {Phone}-based {Machine} {Translation} {Approaches}},
	volume = {3},
	abstract = {There are many abbreviation and non-standard words in SMS and Twitter messages. They are problematic for text-to-speech (TTS) or language processing techniques for these data. A character-based machine translation (MT) approach was previously used for normalization of non-standard words. In this paper, we propose a two-stage translation method to leverage phonetic information, where non-standard words are first
translated to possible pronunciations, which are then translated to standard words. We further combine it with the single-step character-based translation module. Our experiments show that our proposed method significantly outperforms previous results in both n-best coverage and 1-best accuracy.},
	author = {Li, Chen and Liu, Yang},
	month = sep,
	year = {2012},
	file = {Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\HQBV3JE5\\Li and Liu - 2012 - Normalization of Text Messages Using Character- an.pdf:application/pdf}
}

@article{pennell_normalization_2014,
	title = {Normalization of {Informal} {Text}},
	volume = {28},
	issn = {0885-2308},
	url = {http://dx.doi.org/10.1016/j.csl.2013.07.001},
	doi = {10.1016/j.csl.2013.07.001},
	abstract = {This paper describes a noisy-channel approach for the normalization of informal text, such as that found in emails, chat rooms, and SMS messages. In particular, we introduce two character-level methods for the abbreviation modeling aspect of the noisy channel model: a statistical classifier using language-based features to decide whether a character is likely to be removed from a word, and a character-level machine translation model. A two-phase approach is used; in the first stage the possible candidates are generated using the selected abbreviation model and in the second stage we choose the best candidate by decoding using a language model. Overall we find that this approach works well and is on par with current research in the field.},
	number = {1},
	urldate = {2018-12-28},
	journal = {Comput. Speech Lang.},
	author = {Pennell, Deana L. and Liu, Yang},
	month = jan,
	year = {2014},
	keywords = {NLP applications, Noisy text, Text normalization},
	pages = {256--277}
}

@article{bisani_joint-sequence_2008,
	title = {Joint-sequence models for grapheme-to-phoneme conversion},
	volume = {50},
	issn = {0167-6393},
	url = {http://www.sciencedirect.com/science/article/pii/S0167639308000046},
	doi = {10.1016/j.specom.2008.01.002},
	abstract = {Grapheme-to-phoneme conversion is the task of finding the pronunciation of a word given its written form. It has important applications in text-to-speech and speech recognition. Joint-sequence models are a simple and theoretically stringent probabilistic framework that is applicable to this problem. This article provides a self-contained and detailed description of this method. We present a novel estimation algorithm and demonstrate high accuracy on a variety of databases. Moreover, we study the impact of the maximum approximation in training and transcription, the interaction of model size parameters, n-best list generation, confidence measures, and phoneme-to-grapheme conversion. Our software implementation of the method proposed in this work is available under an Open Source license.},
	number = {5},
	urldate = {2019-01-03},
	journal = {Speech Communication},
	author = {Bisani, Maximilian and Ney, Hermann},
	month = may,
	year = {2008},
	keywords = {Grapheme-to-phoneme, Joint-sequence model, Letter-to-sound, Phonemic transcription, Pronunciation modeling},
	pages = {434--451},
	file = {ScienceDirect Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\VV3YWP8W\\S0167639308000046.html:text/html;Submitted Version:C\:\\Users\\jaspe\\Zotero\\storage\\6Z44XY3I\\Bisani and Ney - 2008 - Joint-sequence models for grapheme-to-phoneme conv.pdf:application/pdf}
}

@article{toshniwal_jointly_2016,
	title = {Jointly {Learning} to {Align} and {Convert} {Graphemes} to {Phonemes} with {Neural} {Attention} {Models}},
	url = {http://arxiv.org/abs/1610.06540},
	abstract = {We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).},
	urldate = {2019-01-03},
	journal = {arXiv:1610.06540 [cs]},
	author = {Toshniwal, Shubham and Livescu, Karen},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.06540},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1610.06540 PDF:C\:\\Users\\jaspe\\Zotero\\storage\\MGCFDG9P\\Toshniwal and Livescu - 2016 - Jointly Learning to Align and Convert Graphemes to.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\7IX9MSNS\\1610.html:text/html}
}

@inproceedings{papineni_bleu:_2002,
	address = {Stroudsburg, PA, USA},
	series = {{ACL} '02},
	title = {{BLEU}: {A} {Method} for {Automatic} {Evaluation} of {Machine} {Translation}},
	shorttitle = {{BLEU}},
	url = {https://doi.org/10.3115/1073083.1073135},
	doi = {10.3115/1073083.1073135},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
	urldate = {2019-01-03},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	year = {2002},
	pages = {311--318},
	file = {ACM Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\I4DG7MQ6\\Papineni et al. - 2002 - BLEU A Method for Automatic Evaluation of Machine.pdf:application/pdf}
}

@article{post_call_2018,
	title = {A {Call} for {Clarity} in {Reporting} {BLEU} {Scores}},
	url = {http://arxiv.org/abs/1804.08771},
	abstract = {The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to "the" BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SacreBLEU, to facilitate this.},
	urldate = {2019-01-03},
	journal = {arXiv:1804.08771 [cs]},
	author = {Post, Matt},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.08771},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1804.08771 PDF:C\:\\Users\\jaspe\\Zotero\\storage\\KL77E2FP\\Post - 2018 - A Call for Clarity in Reporting BLEU Scores.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\J8EMTIN7\\1804.html:text/html}
}

@misc{halma_arvid_nodate,
	title = {arvid / chitchat},
	url = {https://bitbucket.org/arvid/chitchat},
	abstract = {Script information retrieval and conversations},
	language = {en},
	urldate = {2019-01-03},
	author = {Halma, Arvid},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\MFRKKTEC\\overview.html:text/html}
}

@inproceedings{jaitly_rnn_2017,
	title = {An {RNN} {Model} of {Text} {Normalization}},
	url = {https://arxiv.org/pdf/1611.00068.pdf},
	urldate = {2019-01-03},
	author = {Jaitly, Navdeep and Sproat, Richard},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\W8QUSVMN\\Jaitly and Sproat - 2017 - An RNN Model of Text Normalization.pdf:application/pdf}
}

@inproceedings{lopez-ludena_architecture_2012,
	title = {Architecture for {Text} {Normalization} using {Statistical} {Machine} {Translation} techniques},
	abstract = {This paper proposes an architecture, based on statistical machine translation, for developing the text normalization module of a text to speech conversion system. The main target is to generate a language independent text normalization module, based on data and flexible enough to deal with all situations presented in this task. The proposed architecture is composed by three main modules: a tokenizer module for splitting the text input into a token graph (tokenization), a phrase-based translation module (token translation) and a postprocessing module for removing some tokens. This paper presents initial experiments for numbers and abbreviations. The very good results obtained validate the proposed architecture.},
	author = {López-Ludeña, Verónica and San-Segundo, Rubén and Montero, J. M. and Barra-Chicote, Roberto and Lorenzo, Javier Mateos},
	year = {2012},
	keywords = {Abbreviations, Experiment, Lexical analysis, Speech synthesis, Statistical machine translation, Text normalization, Tokenization (data security)},
	file = {Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\5BNJKXGF\\López-Ludeña et al. - 2012 - Architecture for Text Normalization using Statisti.pdf:application/pdf}
}

@inproceedings{satapathy_phonetic-based_2017,
	title = {Phonetic-{Based} {Microtext} {Normalization} for {Twitter} {Sentiment} {Analysis}},
	doi = {10.1109/ICDMW.2017.59},
	abstract = {The proliferation of Web 2.0 technologies and the increasing use of computer-mediated communication resulted in a new form of written text, termed microtext. This poses new challenges to natural language processing tools which are usually designed for well-written text. This paper proposes a phonetic-based framework for normalizing microtext to plain English and, hence, improve the classification accuracy of sentiment analysis. Results demonstrated that there is a high ({\textgreater}0.8) similarity index between tweets normalized by our model and tweets normalized by human annotators in 85.31\% of cases, and that there is an accuracy increase of {\textgreater}4\% in terms of polarity detection after normalization.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Data} {Mining} {Workshops} ({ICDMW})},
	author = {Satapathy, R. and Guerreiro, C. and Chaturvedi, I. and Cambria, E.},
	month = nov,
	year = {2017},
	keywords = {classification accuracy, computer mediated communication, Error correction, Knowledge based systems, Microtext analysis, microtext normalization, natural language processing tools, pattern classification, phonetic-based framework, plain English, Semantics, sentiment analysis, Sentiment analysis, similarity index, social networking (online), speech processing, Terminology, Text normalization, tweets, Twitter, twitter sentiment analysis},
	pages = {407--413},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\jaspe\\Zotero\\storage\\FDZSLCMH\\8215691.html:text/html}
}

@inproceedings{khoury_microtext_2015,
	title = {Microtext normalization using probably-phonetically-similar word discovery},
	doi = {10.1109/WiMOB.2015.7347988},
	abstract = {Microtext normalization is the challenge of discovering the English words corresponding to the unusually-spelled words used in social-media messages and posts. In this paper, we propose a novel method for doing this by rendering both English and microtext words phonetically based on their spelling, and matching similar ones together. We present our algorithm to learn spelling-to-phonetic probabilities and to efficiently search the English language and match words together. Our results demonstrate that our system correctly handles many types of normalization problems.},
	booktitle = {2015 {IEEE} 11th {International} {Conference} on {Wireless} and {Mobile} {Computing}, {Networking} and {Communications} ({WiMob})},
	author = {Khoury, R.},
	month = oct,
	year = {2015},
	keywords = {Conferences, Dictionaries, Electronic publishing, Encyclopedias, English language, Internet, linguistics, microtext, microtext normalization, microtext word, normalization, phonetic, probability, rendering, social media, social networking (online), social-media message, speech processing, spelling-to-phonetic probability, text analysis, wiktionary, word discovery},
	pages = {384--391},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\jaspe\\Zotero\\storage\\Q6JLSE6Z\\7347988.html:text/html}
}

@article{chrupala_normalizing_nodate,
	title = {Normalizing tweets with edit scripts and recurrent neural embeddings},
	url = {http://citeseerx.ist.psu.edu/viewdoc/citations;jsessionid=3C665FAF5D29C92C6D8CB06DFF03EC79?doi=10.1.1.652.9864},
	abstract = {CiteSeerX - Document Details (Isaac Councill, Lee Giles, Pradeep Teregowda): Tweets often contain a large proportion of abbreviations, alternative spellings, novel words and other non-canonical language. These features are problematic for stan-dard language analysis tools and it can be desirable to convert them to canoni-cal form. We propose a novel text nor-malization model based on learning edit operations from labeled data while incor-porating features induced from unlabeled data via character-level neural text embed-dings. The text embeddings are generated using an Simple Recurrent Network. We find that enriching the feature set with text embeddings substantially lowers word er-ror rates on an English tweet normaliza-tion dataset. Our model improves on state-of-the-art with little training data and with-out any lexical resources. 1},
	language = {en},
	urldate = {2019-01-03},
	author = {Chrupała, Grzegorz},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\S7W8GQRK\\citations\;jsessionid=3C665FAF5D29C92C6D8CB06DFF03EC79.html:text/html}
}

@inproceedings{min_ncsu_sas_wookhee:_2015,
	title = {{NCSU}\_SAS\_WOOKHEE: {A} {Deep} {Contextual} {Long}-{Short} {Term} {Memory} {Model} for {Text} {Normalization}},
	shorttitle = {{NCSU}\_SAS\_WOOKHEE},
	doi = {10.18653/v1/W15-4317},
	abstract = {To address the challenges of normalizing online conversational texts prevalent in social media, we propose a contextual long-short term memory (LSTM) recurrent neural network based approach, augmented with a self-generated dictionary normalization technique. Our approach utilizes a sequence of characters as well as the part-of-speech associated with words without harnessing any external lexical resources. This work is evaluated on the English Tweet data set provided by the ACL 2015 W-NUT Normalization of Noisy Text shared task. The results, by achieving second place (F1 score: 81.75\%) in the constrained track of the competition, indicate that the proposed LSTM-based approach is a promising tool for normalizing non-standard language.},
	booktitle = {{NUT}@{IJCNLP}},
	author = {Min, Wookhee and Mott, Bradford W.},
	year = {2015},
	keywords = {Artificial neural network, Dictionary, F1 score, Long short-term memory, Noisy text, Recurrent neural network, Social media, Text normalization},
	file = {Full Text PDF:C\:\\Users\\jaspe\\Zotero\\storage\\68WEDSXB\\Min and Mott - 2015 - NCSU_SAS_WOOKHEE A Deep Contextual Long-Short Ter.pdf:application/pdf}
}

@book{bird_natural_2009,
	title = {Natural {Language} {Processing} with {Python}},
	isbn = {978-0-596-51649-9},
	author = {Bird, Steven and Klein, Ewan and Loper, Edward},
	month = jan,
	year = {2009}
}

@misc{noauthor_evaluating_nodate,
	title = {Evaluating models {\textbar} {AutoML} {Translation}},
	url = {https://cloud.google.com/translate/automl/docs/evaluate},
	language = {en},
	urldate = {2019-01-03},
	journal = {Google Cloud},
	file = {Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\L266Q43L\\evaluate.html:text/html}
}

@article{lamb_professor_2016,
	title = {Professor {Forcing}: {A} {New} {Algorithm} for {Training} {Recurrent} {Networks}},
	shorttitle = {Professor {Forcing}},
	url = {http://arxiv.org/abs/1610.09038},
	abstract = {The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.},
	urldate = {2019-01-03},
	journal = {arXiv:1610.09038 [cs, stat]},
	author = {Lamb, Alex and Goyal, Anirudh and Zhang, Ying and Zhang, Saizheng and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.09038},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1610.09038 PDF:C\:\\Users\\jaspe\\Zotero\\storage\\TWRNNPJV\\Lamb et al. - 2016 - Professor Forcing A New Algorithm for Training Re.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\jaspe\\Zotero\\storage\\QP3A3FCY\\1610.html:text/html}
}